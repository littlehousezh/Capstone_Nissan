{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef062111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver 4.1\n",
    "# Added the ability to search months until the end of the year\n",
    "\n",
    "# Reddit Scraper Version 4.0\n",
    "# The scraper can now repeatedly search until user chooses to quit\n",
    "# Added the function to search for a particular month's posts\n",
    "# Addressed the issue of not being able to search for December's posts\n",
    "# Added attempt to resolve request error code 504 and error code 500\n",
    "# Improved readibility of the debugging information\n",
    "# Improved human conversational element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73d339d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in the authentication info(auth, data and headers), returning the access token as string\n",
    "def get_new_token(auth, data, headers):\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data = data, headers=headers)\n",
    "    return res.json()['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psaw\n",
    "from psaw import PushshiftAPI\n",
    "import praw\n",
    "import requests\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "# Introducing developer account information (AUTH)\n",
    "client_id = 'wqZ1aK0ueJCbMnCLGk-3xw' #'lpwG9hNI1cfGJavSouwYQA'\n",
    "\n",
    "client_secret = 'nmHMxN5gpdgDN_Wx_uBuInW5Yj50DQ' # 'hep0ZpXz11bCylZriIjvJIbVTfmAdw'\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "user_agent =\"Advisor\" \n",
    "\n",
    "# The account information (DATA)\n",
    "data = {\n",
    "'grant_type': 'password',\n",
    "'username': 'Boring-Country3996',\n",
    "'password': 'Yoona3377585sh$',\n",
    "}\n",
    "\n",
    "# Setting up the pre-authorized headers (HEADERS)\n",
    "pre_authorized_headers = {'User-Agent': user_agent}\n",
    "\n",
    "# Get access token with AUTH, DATA, and HEADERS\n",
    "token = get_new_token(auth, data, pre_authorized_headers)\n",
    "\n",
    "# Update the authorized headers\n",
    "headers = pre_authorized_headers\n",
    "headers['Authorization'] = 'bearer {}'.format(token)\n",
    "\n",
    "# Setting up a PRAW reddit instance\n",
    "r = praw.Reddit(client_id=client_id,\n",
    "                client_secret=client_secret,\n",
    "                user_agent=client_secret)\n",
    "\n",
    "# Setting up a Pushshift API with PRAW reddit instance to obtain the most updated data\n",
    "api=PushshiftAPI(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7dd4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters the submission such that deleted posts and removed posts are excluded\n",
    "# Returns the list of \"meaningful\" urls\n",
    "def filter_submission(raw_posts):\n",
    "    alpha_urls = []\n",
    "    for submission in raw_posts:\n",
    "        if ((submission.selftext != '[removed]') and (submission.selftext != '[deleted]')):\n",
    "            alpha_urls.append(submission.url)\n",
    "    \n",
    "    return alpha_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f174a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get posts from a certain subreddit within a specified period of time\n",
    "def get_posts_for_time_period(sub, after, before):\n",
    "    print(\"Querying pushshift...\")\n",
    "    result = list(api.search_submissions(subreddit=sub,\n",
    "                                     after=after,\n",
    "                                     before=before))\n",
    "    \n",
    "    print(\"Single query completed.\")\n",
    "    return filter_submission(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d2c9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Retrieving the posts from a specific subreddit within a specified time period\n",
    "# Start date should be a more recent date compared to the end date\n",
    "# Start date is non-inclusive, while the end date is inclusive. (Will collect data on end date and day before start date)\n",
    "# Searching backwards with regard of time\n",
    "def retrieve_subs_time(start, end, sub):\n",
    "    print(\"Starting the pushshift searching process between\", datetime.fromtimestamp(end), \"and\", datetime.fromtimestamp(start))\n",
    "    day_in_second = 86400\n",
    "    \n",
    "    # The more recent date\n",
    "    before_timestamp = start\n",
    "    \n",
    "    # The more ancient date\n",
    "    after_timestamp = start - day_in_second\n",
    "    \n",
    "    print(\"Now searching for posts between\", datetime.fromtimestamp(after_timestamp), \"and\", datetime.fromtimestamp(before_timestamp))\n",
    "    data = get_posts_for_time_period(sub, after_timestamp, before_timestamp)\n",
    "    all_data = data\n",
    "    after_timestamp -= day_in_second\n",
    "    before_timestamp -= day_in_second\n",
    "    \n",
    "    while(before_timestamp > end):\n",
    "        time.sleep(2)\n",
    "        print(\"Now searching for posts between\", datetime.fromtimestamp(after_timestamp), \"and\", datetime.fromtimestamp(before_timestamp))\n",
    "        data = get_posts_for_time_period(sub, after_timestamp, before_timestamp)\n",
    "        all_data.extend(data)\n",
    "        after_timestamp -= day_in_second\n",
    "        before_timestamp -= day_in_second\n",
    "        \n",
    "    print(\"Done with pushshift searching between\", datetime.fromtimestamp(end), \"and\", datetime.fromtimestamp(start))\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "517ba0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "average_request_period = 1\n",
    "\n",
    "# Collects the post data in json format. Takes in a list of urls and the headers information\n",
    "def get_post_json(urls, headers, auth, data, pre_authorized_headers):\n",
    "    \n",
    "    temp_collection = []\n",
    "    print(\"Requesting the JSON object based on urls...\")\n",
    "    print(\"There are\", len(urls), \"submissions to collect this time.\")\n",
    "    count_collected = 0\n",
    "    start_at = time.time()\n",
    "    time_mark = start_at\n",
    "    \n",
    "    for post_url in urls:\n",
    "        \n",
    "        # Adjust the Reddit url such that it leads to the website via oauth\n",
    "        oauth_url = 'https://oauth.reddit.com' + post_url[22:]\n",
    "        \n",
    "        pre_request_time = time.time()\n",
    "        try:\n",
    "            # Send the request to get json on two-second-per-request manner\n",
    "            request = requests.get(oauth_url, headers = headers)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print ('Exception encountered:', e.__class__.__name__)\n",
    "            if (e.__class__.__name__ == 'TimeoutError'):\n",
    "                print(\"Timeout exception\")\n",
    "                print(\"Current url is:\",  post_url)\n",
    "                print(\"Oauth url is:\", oauth_url)\n",
    "                print(\"Retrying...\")\n",
    "                request = requests.get(oauth_url, headers = headers)\n",
    "            if (e.__class__.__name__ == 'ConnectionError'):\n",
    "                print(\"Connection Error exception\")\n",
    "                print(\"Current url is:\",  post_url)\n",
    "                print(\"Oauth url is:\", oauth_url)\n",
    "                print(\"Retrying...\")\n",
    "                \n",
    "                # Extra layer of safety\n",
    "                try:\n",
    "                    request = requests.get(oauth_url, headers = headers)\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print ('Exception encountered:', e.__class__.__name__)\n",
    "                    if (e.__class__.__name__ == 'TimeoutError'):\n",
    "                        print(\"Timeout exception\")\n",
    "                        print(\"Current url is:\",  post_url)\n",
    "                        print(\"Oauth url is:\", oauth_url)\n",
    "                        print(\"Retrying...\")\n",
    "                        request = requests.get(oauth_url, headers = headers)\n",
    "                    if (e.__class__.__name__ == 'ConnectionError'):\n",
    "                        print(\"Connection Error exception\")\n",
    "                        print(\"Current url is:\",  post_url)\n",
    "                        print(\"Oauth url is:\", oauth_url)\n",
    "                        print(\"Retrying...\")\n",
    "                        request = requests.get(oauth_url, headers = headers)\n",
    "        \n",
    "        if (time.time() - pre_request_time > 10):\n",
    "            print(\"The scraping process paused for\", time.time() - pre_request_time, \"second. Now resumed.\")\n",
    "        \n",
    "        # Check if the request is still valid. Retrieve a new token if necessary (get status code 401).\n",
    "        if (request.status_code == 401):\n",
    "            print(\"NOTE: Error code 401/unauthorized is displayed. The access token is likely expired\")\n",
    "            print(\"Reauthorizing and retrieving new access token...\")\n",
    "            token = get_new_token(auth, data, pre_authorized_headers)\n",
    "            headers['Authorization'] = 'bearer {}'.format(token)\n",
    "            request = requests.get(oauth_url, headers = headers)\n",
    "            \n",
    "            if (request.status_code == 200):\n",
    "                print(\"Request now returns status code 200. Should be good to go\")\n",
    "        \n",
    "        # If status code is 429, wait for the ratelimit to reset\n",
    "        if (request.status_code == 429):\n",
    "            print(\"NOTE: Error code 429/too many requests is displayed. The next reset is:\",\n",
    "                  request.headers['x-ratelimit-reset'], 'second')\n",
    "            print(\"Waiting for the ratelimit to reset...\")\n",
    "            time.sleep(request.headers['x-ratelimit-reset'])\n",
    "            request = requests.get(oauth_url, headers = headers)\n",
    "            \n",
    "            if (request.status_code == 200):\n",
    "                print(\"Request now returns status code 200. Should be good to go\")\n",
    "        \n",
    "        # If status code is 504, retry the request. If the status code is still 504, skip this particular url.\n",
    "        if (request.status_code == 504):\n",
    "            print(\"NOTE: Error code 504/Gateway_Timeout is displayed.\")\n",
    "            print(\"Current post url is:\", post_url)\n",
    "            print(\"Attempting to sleep for 2 second and retry\")\n",
    "            time.sleep(2)\n",
    "            request = requests.get(oauth_url, headers = headers)\n",
    "            if (request.status_code == 504):\n",
    "                print(\"NOTE: Request still gets error code 504. Attempting to skip this url.\")\n",
    "                break\n",
    "            \n",
    "            if (request.status_code == 200):\n",
    "                print(\"Request now returns status code 200. Should be good to go\")\n",
    "                \n",
    "        # If status code is 500, retry the request. If the status code is still 504, skip this particular url.\n",
    "        if (request.status_code == 500):\n",
    "            print(\"NOTE: Error code 500/Internal_Server_Error is displayed.\")\n",
    "            print(\"Current post url is:\", post_url)\n",
    "            print(\"Attempting to sleep for 2 second and retry\")\n",
    "            time.sleep(2)\n",
    "            request = requests.get(oauth_url, headers = headers)\n",
    "            if (request.status_code == 500):\n",
    "                print(\"NOTE: Request still gets error code 500. Attempting to skip this url.\")\n",
    "                break\n",
    "            \n",
    "            if (request.status_code == 200):\n",
    "                print(\"Request now returns status code 200. Should be good to go\")\n",
    "        \n",
    "        # If the status code is not seen, I will let the program end to see the error.\n",
    "        if (request.status_code != 200):\n",
    "            print(\"NOTE: unprecedented status code returned. The code is:\", request.status_code)\n",
    "            print(\"Please check the developer interface of request module documentation\")\n",
    "            print(\"Current post url is:\", post_url)\n",
    "        \n",
    "        # Check if the ratelimit is met. If so, sleep until ratelimit resets\n",
    "        if(request.headers['x-ratelimit-remaining'] == 0):\n",
    "            print(\"The ratelimit has been reached for this time period. Pause for\", request.headers['x-ratelimit-reset'], \"second\")\n",
    "            time.sleep(request.headers['x-ratelimit-reset'])\n",
    "        # If the ratelimit is not met, optimize the rate of sending requests\n",
    "        # If the remaining requests allowed is less than reset, sleep for 1 second for each request\n",
    "        elif (request.headers['x-ratelimit-remaining'] <= request.headers['x-ratelimit-reset']):\n",
    "            time.sleep(average_request_period)\n",
    "        # If the remaining requests allowed is greater than reset, send request without pausing program\n",
    "        \n",
    "        # Store the request in json format with type 'str', then append it to the list\n",
    "        post_json = request.json()\n",
    "        temp_collection.append(post_json)\n",
    "        \n",
    "        # Debugging information\n",
    "        count_collected += 1\n",
    "        if ((count_collected % 100) == 0):\n",
    "            set_completion_time = time.time() - time_mark\n",
    "            time_mark = time.time()\n",
    "            print(\"Complete collecting a set of 100 posts. Time taken for this set is:\", set_completion_time, \"second\")\n",
    "            print(\"Total number of posts collected so far:\", count_collected)\n",
    "            print(\"Number of remaining posts to collect:\", len(urls) - count_collected)\n",
    "            print(\"Progress ratio(%):\", ((count_collected/len(urls)) * 100))\n",
    "            print(\"Ratelimit conditions:\")\n",
    "            print(\"x-ratelimit-remaining:\", request.headers['x-ratelimit-remaining'], \"requests\")\n",
    "            print(\"x-ratelimit-used:\", request.headers['x-ratelimit-used'], \"requests\")\n",
    "            print(\"x-ratelimit-reset in:\", request.headers['x-ratelimit-reset'], \"seconds\")\n",
    "            print(\"\")\n",
    "        \n",
    "    print(\"Done with the JSON object collection.\")\n",
    "    print(\"Total time spent:\", time.time() - start_at, \"second, which is\", (time.time() - start_at) / 60, \"minutes\")\n",
    "\n",
    "    return temp_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99eaf4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for the monthly data if it does not exist\n",
    "def create_folder(year, month):\n",
    "    folder_name = str(year) + \"_\" + str(month)\n",
    "    path = '/home/haonan/relationship_advisor_data/' + folder_name\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "        print(\"path:\", path, \"created\")\n",
    "        \n",
    "    return (path + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ea0c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json_file(post, directory):\n",
    "    # Create json file\n",
    "    data = json.dumps(post, indent = 6, sort_keys = True)\n",
    "    \n",
    "    path = directory + post[0]['data']['children'][0]['data']['name'] + \".json\"\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, sort_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "635bc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from calendar import monthrange\n",
    "\n",
    "# A prototype for get posts by year\n",
    "def get_post_by_month(year, month):\n",
    "    \n",
    "    print(\"\\nStarting the search for the posts in month\", month, \"in year\", year, \"\\n\")\n",
    "    \n",
    "    # Create a folder for the monthly data if it does not exist\n",
    "    folder_path = create_folder(year, month)\n",
    "    \n",
    "    if (month == 12):\n",
    "        start_time = int(dt.datetime(year + 1, 1, 1).timestamp())\n",
    "    else:\n",
    "        start_time = int(dt.datetime(year, month + 1, 1).timestamp())\n",
    "        \n",
    "    end_time = int(dt.datetime(year, month, 1).timestamp())\n",
    "    \n",
    "    post_urls = retrieve_subs_time(start_time, end_time, 'relationships')\n",
    "    posts_json = get_post_json(post_urls, headers, auth, data, pre_authorized_headers)\n",
    "    \n",
    "    for post in posts_json:\n",
    "        write_json_file(post, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c883c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_by_year(year):\n",
    "    print(\"\\nStarting the search for the posts in year\", year,\"\\n\")\n",
    "    for i in range (12):\n",
    "        get_post_by_month(year, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "595cd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiation():    \n",
    "    search_preference = input((\"Would you like to search by year or search by month? Enter year or month:\\n\")).lower()\n",
    "    if (search_preference == 'year'):\n",
    "        target_year = int(input(\"Which year would you like to search?\\n\"))\n",
    "        get_post_by_year(target_year)\n",
    "    elif (search_preference == 'month'):\n",
    "        target_year = int(input(\"Gotcha. First tell me which year?\\n\"))\n",
    "        target_month = int(input(\"Understood. Please tell me which month in that year? Enter integer between 1 and 12:\\n\"))\n",
    "        month_search_preference = input((\"Got it. Would you like to start from this month and search til the end of the year? Press y for yes and n for no:\\n\")).lower()\n",
    "        if (month_search_preference == 'y'):\n",
    "            for i in range (12):\n",
    "                if (i + 1 < target_month):\n",
    "                    continue\n",
    "                else:\n",
    "                    get_post_by_month(target_year, i + 1)\n",
    "        else:\n",
    "            get_post_by_month(target_year, target_month)\n",
    "    else:\n",
    "        print(search_preference, \"is not a valid input. Please try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1aa2c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Reddit Scraper version 4.0!\n",
      "\n",
      "Would you like to search by year or search by month? Enter year or month:\n",
      "sakdjk\n",
      "sakdjk is not a valid input. Please try again\n",
      "\n",
      "Completed one target search. Do you want to do another search? Enter y as yes and enter q as quit:\n",
      "h\n",
      "Invalid input. Please try again.\n",
      "\n",
      "Do you want to do another search? Enter y as yes and enter q as quit:\n",
      "y\n",
      "Would you like to search by year or search by month? Enter year or month:\n",
      "month\n",
      "Gotcha. First tell me which year?2021\n",
      "Understood. Please tell me which month in that year?12\n",
      "path: /home/markus/relationship_advisor_data/2021_12 created\n",
      "Starting the pushshift searching process between 2021-12-01 00:00:00 and 2022-01-01 00:00:00\n",
      "Now searching for posts between 2021-12-31 00:00:00 and 2022-01-01 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-30 00:00:00 and 2021-12-31 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-29 00:00:00 and 2021-12-30 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-28 00:00:00 and 2021-12-29 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-27 00:00:00 and 2021-12-28 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-26 00:00:00 and 2021-12-27 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-25 00:00:00 and 2021-12-26 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-24 00:00:00 and 2021-12-25 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-23 00:00:00 and 2021-12-24 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-22 00:00:00 and 2021-12-23 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-21 00:00:00 and 2021-12-22 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-20 00:00:00 and 2021-12-21 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-19 00:00:00 and 2021-12-20 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-18 00:00:00 and 2021-12-19 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-17 00:00:00 and 2021-12-18 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-16 00:00:00 and 2021-12-17 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-15 00:00:00 and 2021-12-16 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-14 00:00:00 and 2021-12-15 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-13 00:00:00 and 2021-12-14 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-12 00:00:00 and 2021-12-13 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-11 00:00:00 and 2021-12-12 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-10 00:00:00 and 2021-12-11 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-09 00:00:00 and 2021-12-10 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-08 00:00:00 and 2021-12-09 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-07 00:00:00 and 2021-12-08 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-06 00:00:00 and 2021-12-07 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-05 00:00:00 and 2021-12-06 00:00:00\n",
      "Querying pushshift...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markus/.local/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/home/markus/.local/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single query completed.\n",
      "Now searching for posts between 2021-12-04 00:00:00 and 2021-12-05 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-03 00:00:00 and 2021-12-04 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-02 00:00:00 and 2021-12-03 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Now searching for posts between 2021-12-01 00:00:00 and 2021-12-02 00:00:00\n",
      "Querying pushshift...\n",
      "Single query completed.\n",
      "Done with pushshift searching between 2021-12-01 00:00:00 and 2022-01-01 00:00:00\n",
      "Requesting the JSON object based on urls...\n",
      "There are 2020 submissions to collect this time.\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 63.3474805355072 second\n",
      "Total number of posts collected so far: 100\n",
      "Number of remaining posts to collect: 1920\n",
      "Progress ratio(%): 4.9504950495049505\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 500.0 requests\n",
      "x-ratelimit-used: 100 requests\n",
      "x-ratelimit-reset in: 34 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 49.35365891456604 second\n",
      "Total number of posts collected so far: 200\n",
      "Number of remaining posts to collect: 1820\n",
      "Progress ratio(%): 9.900990099009901\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 585.0 requests\n",
      "x-ratelimit-used: 15 requests\n",
      "x-ratelimit-reset in: 586 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 100.20961952209473 second\n",
      "Total number of posts collected so far: 300\n",
      "Number of remaining posts to collect: 1720\n",
      "Progress ratio(%): 14.85148514851485\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 485.0 requests\n",
      "x-ratelimit-used: 115 requests\n",
      "x-ratelimit-reset in: 486 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 99.69849634170532 second\n",
      "Total number of posts collected so far: 400\n",
      "Number of remaining posts to collect: 1620\n",
      "Progress ratio(%): 19.801980198019802\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 385.0 requests\n",
      "x-ratelimit-used: 215 requests\n",
      "x-ratelimit-reset in: 385 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 99.68022274971008 second\n",
      "Total number of posts collected so far: 500\n",
      "Number of remaining posts to collect: 1520\n",
      "Progress ratio(%): 24.752475247524753\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 285.0 requests\n",
      "x-ratelimit-used: 315 requests\n",
      "x-ratelimit-reset in: 285 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 99.9161651134491 second\n",
      "Total number of posts collected so far: 600\n",
      "Number of remaining posts to collect: 1420\n",
      "Progress ratio(%): 29.7029702970297\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 185.0 requests\n",
      "x-ratelimit-used: 415 requests\n",
      "x-ratelimit-reset in: 185 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 100.01359272003174 second\n",
      "Total number of posts collected so far: 700\n",
      "Number of remaining posts to collect: 1320\n",
      "Progress ratio(%): 34.65346534653465\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 85.0 requests\n",
      "x-ratelimit-used: 515 requests\n",
      "x-ratelimit-reset in: 85 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 101.18566107749939 second\n",
      "Total number of posts collected so far: 800\n",
      "Number of remaining posts to collect: 1220\n",
      "Progress ratio(%): 39.603960396039604\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 584.0 requests\n",
      "x-ratelimit-used: 16 requests\n",
      "x-ratelimit-reset in: 585 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 100.66777873039246 second\n",
      "Total number of posts collected so far: 900\n",
      "Number of remaining posts to collect: 1120\n",
      "Progress ratio(%): 44.554455445544555\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 484.0 requests\n",
      "x-ratelimit-used: 116 requests\n",
      "x-ratelimit-reset in: 485 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 99.42938923835754 second\n",
      "Total number of posts collected so far: 1000\n",
      "Number of remaining posts to collect: 1020\n",
      "Progress ratio(%): 49.504950495049506\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 384.0 requests\n",
      "x-ratelimit-used: 216 requests\n",
      "x-ratelimit-reset in: 384 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 99.89875745773315 second\n",
      "Total number of posts collected so far: 1100\n",
      "Number of remaining posts to collect: 920\n",
      "Progress ratio(%): 54.45544554455446\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 284.0 requests\n",
      "x-ratelimit-used: 316 requests\n",
      "x-ratelimit-reset in: 284 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 99.89684963226318 second\n",
      "Total number of posts collected so far: 1200\n",
      "Number of remaining posts to collect: 820\n",
      "Progress ratio(%): 59.4059405940594\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 184.0 requests\n",
      "x-ratelimit-used: 416 requests\n",
      "x-ratelimit-reset in: 184 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 100.01735186576843 second\n",
      "Total number of posts collected so far: 1300\n",
      "Number of remaining posts to collect: 720\n",
      "Progress ratio(%): 64.35643564356435\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 84.0 requests\n",
      "x-ratelimit-used: 516 requests\n",
      "x-ratelimit-reset in: 84 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 101.49972987174988 second\n",
      "Total number of posts collected so far: 1400\n",
      "Number of remaining posts to collect: 620\n",
      "Progress ratio(%): 69.3069306930693\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 583.0 requests\n",
      "x-ratelimit-used: 17 requests\n",
      "x-ratelimit-reset in: 584 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 99.48957920074463 second\n",
      "Total number of posts collected so far: 1500\n",
      "Number of remaining posts to collect: 520\n",
      "Progress ratio(%): 74.25742574257426\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 483.0 requests\n",
      "x-ratelimit-used: 117 requests\n",
      "x-ratelimit-reset in: 483 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 100.52766466140747 second\n",
      "Total number of posts collected so far: 1600\n",
      "Number of remaining posts to collect: 420\n",
      "Progress ratio(%): 79.20792079207921\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 383.0 requests\n",
      "x-ratelimit-used: 217 requests\n",
      "x-ratelimit-reset in: 383 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 100.18858933448792 second\n",
      "Total number of posts collected so far: 1700\n",
      "Number of remaining posts to collect: 320\n",
      "Progress ratio(%): 84.15841584158416\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 283.0 requests\n",
      "x-ratelimit-used: 317 requests\n",
      "x-ratelimit-reset in: 283 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 99.40001583099365 second\n",
      "Total number of posts collected so far: 1800\n",
      "Number of remaining posts to collect: 220\n",
      "Progress ratio(%): 89.10891089108911\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 183.0 requests\n",
      "x-ratelimit-used: 417 requests\n",
      "x-ratelimit-reset in: 184 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 100.45689368247986 second\n",
      "Total number of posts collected so far: 1900\n",
      "Number of remaining posts to collect: 120\n",
      "Progress ratio(%): 94.05940594059405\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 83.0 requests\n",
      "x-ratelimit-used: 517 requests\n",
      "x-ratelimit-reset in: 84 seconds\n",
      "\n",
      "Complete collecting a set of 100 posts. Time taken for this set is: 100.81965160369873 second\n",
      "Total number of posts collected so far: 2000\n",
      "Number of remaining posts to collect: 20\n",
      "Progress ratio(%): 99.00990099009901\n",
      "Ratelimit conditions:\n",
      "x-ratelimit-remaining: 582.0 requests\n",
      "x-ratelimit-used: 18 requests\n",
      "x-ratelimit-reset in: 583 seconds\n",
      "\n",
      "Done with the JSON object collection.\n",
      "Total time spent: 1935.659702539444 second, which is 32.260995054244994 minutes\n",
      "\n",
      "Completed one target search. Do you want to do another search? Enter y as yes and enter q as quit:\n",
      "q\n",
      "The program is about to quit. Have a good one!\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to Reddit Scraper version 4.0!\\n\")\n",
    "user_indicator = 'y'\n",
    "\n",
    "while (user_indicator == 'y'):\n",
    "    initiation()\n",
    "    user_input = input(\"\\nCompleted one target search. Do you want to do another search? Enter y as yes and enter q as quit:\\n\").lower()\n",
    "    while (user_input != 'y' and user_input != 'q'):\n",
    "        print(\"Invalid input. Please try again.\")\n",
    "        user_input = input(\"\\nDo you want to do another search? Enter y as yes and enter q as quit:\\n\").lower()\n",
    "        \n",
    "    user_indicator = user_input\n",
    "    \n",
    "print(\"The program is about to quit. Have a good one!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa75b4-f8ff-4d52-9eff-d8b3f53a8630",
   "metadata": {},
   "source": [
    "## Reddit Version -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c402256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(client_id='wqZ1aK0ueJCbMnCLGk-3xw',\n",
    "                     client_secret='nmHMxN5gpdgDN_Wx_uBuInW5Yj50DQ',\n",
    "                     username='Boring-Country3996',\n",
    "                     password='Yoona3377585sh$',\n",
    "                     user_agent='Relationship Advisor')\n",
    "\n",
    "subreddit = reddit.subreddit('electricvehicles')\n",
    "\n",
    "# Create dataframe for submissions\n",
    "submissions_data = {'title': [], 'url': []}\n",
    "for submission in subreddit.new(limit=None):\n",
    "    submissions_data['title'].append(submission.title)\n",
    "    submissions_data['url'].append(submission.url)\n",
    "\n",
    "submissions_df = pd.DataFrame(submissions_data)\n",
    "\n",
    "# Create dataframe for comments\n",
    "comments_data = {'body': []}\n",
    "for submission in subreddit.new(limit=None):\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    for comment in submission.comments.list():\n",
    "        comments_data['body'].append(comment.body)\n",
    "\n",
    "comments_df = pd.DataFrame(comments_data)\n",
    "\n",
    "# Save dataframes to CSV\n",
    "submissions_df.to_csv('submissions.csv', index=False)\n",
    "comments_df.to_csv('comments.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4b613-a00c-42be-b590-8757636f05f3",
   "metadata": {},
   "source": [
    "## KBB -version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fba44ea-5249-46a1-96d8-e55cbfed95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-kbb\n",
    "#No pagination function\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.kbb.com/nissan/leaf/2022/consumer-reviews/'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "# Model\n",
    "model = soup.find_all('div', class_='css-tpw6mp e1ma5l2g3')\n",
    "#review title\n",
    "title = soup.find_all('div', class_='css-1c7qqqr')\n",
    "#review content\n",
    "reviews = soup.find_all('p', class_='css-25a2lr emgezi80')\n",
    "#create a list of tuples with the data\n",
    "data = []\n",
    "for i in range(len(model)):\n",
    "    data.append((model[i].get_text(),title[i].get_text(),reviews[i].get_text()))\n",
    "    time.sleep(1)\n",
    "\n",
    "#write the data to a csv file\n",
    "with open('reviews-kbb.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Model\", \"Title\", \"Review\"])\n",
    "    writer.writerows(data)\n",
    "#div data-cy=\"pagination\" class=\"css-1r0my5g e1hywep20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb94a0-953d-4788-9ba6-2c802200c069",
   "metadata": {},
   "source": [
    "## Cars.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec8e1d2a-3d5b-42ae-afc4-a2f6227ccbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.cars.com/research/nissan-leaf-2021/consumer-reviews/\n",
    "#change year and model\n",
    "#Add pagination function\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.cars.com/research/nissan-leaf-2019/consumer-reviews?page_size=200'\n",
    "data = []\n",
    "while url:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # title\n",
    "    title = soup.find_all('h3', class_='sds-heading--7 title')\n",
    "    #review sub-title\n",
    "    sub_title = soup.find_all('div', class_='review-byline review-section')\n",
    "    #review content\n",
    "    reviews = soup.find_all('p', class_='review-body')\n",
    "    #create a list of tuples with the data\n",
    "    \n",
    "    for i in range(len(title)):\n",
    "        data.append((title[i].get_text(),sub_title[i].get_text(),reviews[i].get_text()))\n",
    "        time.sleep(1)\n",
    "    \n",
    "    next_link = soup.find(\"a\",{\"rel\":\"next\"})\n",
    "    if next_link:\n",
    "        url = next_link[\"href\"]\n",
    "    else:\n",
    "        url = None\n",
    "\n",
    "#write the data to a csv file\n",
    "with open('reviews_cars.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Sub-Title\", \"Reviews\"])\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b17847-692d-45aa-a9b2-0e30b648c999",
   "metadata": {},
   "source": [
    "## edmunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c2f5c-8b7e-4f26-a2c4-adc625958581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.edmunds.com/nissan/leaf/2023/consumer-reviews/\n",
    "\n",
    "#Add pagination function\n",
    "\n",
    "##there is some bug\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.edmunds.com/nissan/leaf/2023/consumer-reviews/?pagesize=50'\n",
    "url_list=[]\n",
    "data = []\n",
    "while url:\n",
    "    time.sleep(1)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # title\n",
    "    title = soup.find_all('a', {\"rel\":'nofollow'})\n",
    "    reviews = soup.find_all('p')\n",
    "    #create a list of tuples with the data\n",
    "    for i in range(len(title)):\n",
    "        data.append((title[i].get_text(),reviews[i].get_text()))\n",
    "        \n",
    "    next_link = soup.find(\"a\",{\"aria-label\":\"Go to the next page\"})\n",
    "    \n",
    "    if next_link and next_link[\"href\"] not in url_list:\n",
    "        url = next_link[\"href\"]\n",
    "        url_list.append(url)\n",
    "    else:\n",
    "        url = None\n",
    "\n",
    "#write the data to a csv file\n",
    "with open('reviews_edmunds.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Reviews\"])\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8eed5-195b-4544-9c01-cfb842593498",
   "metadata": {},
   "source": [
    "## consumerreports - require membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd0f38d1-4cfd-4b31-b97b-e02c5e59798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.consumerreports.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b9783-cfe1-4a9a-85ab-c8d4bd77cf81",
   "metadata": {},
   "source": [
    "## nissanzclub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3964c4b-ccb9-4f0c-90e4-95f3ad1420c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.nissanzclub.com/forum/\n",
    "\n",
    "#change model\n",
    "#Add pagination function\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.nissanzclub.com/forum/search/106051/?q=Leaf&o=date'\n",
    "data = []\n",
    "while url:\n",
    "    time.sleep(1)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    next_link = soup.find(\"a\",{\"class\":\"pageNav-jump pageNav-jump--next\"})\n",
    "    # title\n",
    "    title = soup.find_all('h3', class_='contentRow-title')\n",
    "    for i in range(len(title)):\n",
    "        link = title[i].find('a')\n",
    "        url1=\"https://www.nissanzclub.com\"+link[\"href\"]\n",
    "        time.sleep(1)\n",
    "        page = requests.get(url1)\n",
    "        #review sub-title\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        reviews = soup.find_all('div', class_='bbCodeBlock-expandContent')\n",
    "        #create a list of tuples with the data\n",
    "        for i in range(len(reviews)):\n",
    "            data.append((link.get_text(),reviews[i].get_text()))\n",
    "\n",
    "    if next_link:\n",
    "        url = \"https://www.nissanzclub.com\"+ next_link[\"href\"]\n",
    "    else:\n",
    "        url = None\n",
    "\n",
    "#write the data to a csv file\n",
    "with open('reviews_fanclub.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Reviews\"])\n",
    "    writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
